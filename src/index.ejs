<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>




  <d-title>
    <h1>How does the computer become a just superhero?</h1>
    <p>A review of fairness in machine learning </p>
    </d-title>
   
    <d-article>
  <p>
   Super cool machine learning systems should ideally be perceived more as righteous superheroes than as unjust villains. But how do we ensure that our models are being classified and represented as such? Or rather, how do we ensure models to be fair and just in their outcome when they have an impact on humans —  like every other superhero movie, the goal is to save humanity. Keeping it down to earth, this paper gives a broad introduction to the concepts, aspects and challenges of fair machine learning, as well as discussing results from a small survey conducted about people’s perception and comprehension in algorithmic fairness. We use 'superheroes' as an illustrative analogue for fair models and a  <a href="https://www.kaggle.com/claudiodavi/superhero-set">superhero dataset</a>  from Kaggle as an example throughout the paper. The survey showed no significant association between people’s self-reported comprehension and their ability to correctly answer comprehension questions, which motivated the need for this paper. The aim is to provide an overview of current work in fair machine learning and make the reader aware of trade-offs and challenges, such as to know when and how fairness in Machine Learning is an essential concern. The paper, therefore, uses interactive illustration to invite the reader to take a stand and check their comprehension about fairness in algorithms during the read.            
    </p>
   
  <h1>Introduction </h1>
  <h3>Please introduce us </h3>
  <p>
     ... to this new universe of fairness. Data-driven models like machine learning models are more and more applied in society within a range of different areas and tasks from recommender systems [REF] over web search [REF] to criminal courts [REFERENCER]. With the rise of machine learning models, there has been an increased focus of fairness and transparency within the field  <d-cite key=" chouldechova2020snapshot "></d-cite>. Several cases of “unfair” machine learning have also been debated in the media [REF]. One example is a United States criminal risk assessment system, COMPAS, used in courts for parole decisions. The system assigns a risk score of recidivism, i.e. the likelihood of a person to recommit a new crime. It was discovered by ProPublica in 2016 that black people, who truly would not commit a new crime, were more likely to get wrongly assigned a higher risk score than white people, and thereby getting a lesser chance of parole <d-cite key=" angwin2016machine "></d-cite>.
  Another example showed an underrepresentation of women and gender stereotypes in occupations in image search <d-cite key="kay2015unequal"></d-cite>.
  In 2015 hitting the news  with an  <a href="https://www.theverge.com/tldr/2015/4/9/8378745/i-see-white-people">example</a> , where the fact that  the first female to appear on a Google Image search on 'CEO' was a barbie doll dressed in a suit after several rows of white males.
  </p>      
  <h3>Every hero has a history</h3>
  <p>
  … and so does bias in models. The main concern of the above examples of algorithmic bias is not the risk of the models’ failure or understanding the technical reasons behind it. According to Kate Crawford in her keynote <i> The Trouble with Bias</i> at NIPS 2017 <d-cite key=" crawford2017keynote "></d-cite>, it is the fact that the models reproduce a cultural and historian bias present in society. The two cases above seem problematic because they are reproducing respectively discrimination of black people and inequality between genders in the labour market. With machine learning becoming more prevalent in the industry where it affects people’s lives, we are potentially looking at a systematic and automatic way of reproducing discrimination or favouritism <d-cite key=" crawford2017keynote "></d-cite>. Like with super-villains, there is often a <i>history</i>behind the unjustness, which we need to understand to turn super-villains into superheroes. However, the solution to stop unjustness is neither straight forward nor simple. The unanswered question remains: what is a fair model? There can never be an absolute answer to this question since it will change over time and depends on the culture. But a lot of work has been done in research to try to mathematical formalize and address this question  <d-cite key="mitchell2018prediction,gajane2017formalizing"></d-cite>. 
  </p>
  <h3>The plot </h3>
  <p>
    … of this paper will circulate the question of what a fair model is by presenting an illustrative introduction to fairness in machine learning. The first scenes have already introduced the theme, and from now, the storyline will take us through why bias occurs, how fairness can be defined and achieved and all the way to perceived algorithmic fairness - with all the challenges and bumps on the way.  Especially, there will be a focus on explaining the following three mathematical fairness criteria: 1) Demographic parity, 2) Equalized Opportunity and 3) Statistical Parity. The paper examines both the comprehension and the perception of fairness with a starting point in these criteria by presenting results from a small survey conducted with 92 participants.  The questions from the survey will be used to guide the reading of this paper. </p> 
  <p>The survey, like this paper, uses a play setting about super figures as an example. We chose this fictional setting both to give a story easy to relate to and to avoid a true or realistic case to which people might be biased from the beginning, such that the essence of the survey can be algorithmic fairness in general and not the actual case. The storyline is formulated along the lines of a set of super figures, who can be either heroes or villains, all wanting to go to a party: they are assessed at the door, e.g. by an algorithm, since only the figures who are “believed” to be heroes are allowed in. The focal question is how to ensure that both the group of male and female figures are treated fair, as well as what fairness means in this setting. The survey details are outlined further in <a href="# Appendix-A"> Appendix A</a>, and the infobox below summarises some brief details and introduces the point of the interactive questionnaires in the paper.
    </p>
   
  <div id="survey-info" class="subgrid"></div>
   
  <p>
  Data from the<a href="https://www.kaggle.com/claudiodavi/superhero-set"> Kaggle superhero dataset</a>is used during the article as an example. The Kaggle dataset is a collection of information about super figures extracted from the <a href="https://www.superherodb.com/"> Superhero Database</a> . A simple classification model has been trained on the dataset to distinguish between villains and heroes where features such as superpowers, names, publisher, height and weight have been accessible. Read more about pre-processing, feature selection and model training in <a href="# Appendix-B"> Appendix B</a>.
  The interactive illustrations and questionnaires invite the reader to take a stand on the fairness questions during the read. You can start with the following:
  </p>
  <div id="questionnaire-target-0" ></div>
  <p>The main contributions of this paper are two-folded. First, it summarises the current state of fairness in machine learning through a literature review presented as an interactive paper encouraging people to think about the issues and challenges with algorithmic fairness.  Second, it examines through a survey people’s comprehension, self-reported understanding  and perception of different fairness criteria in the superhero setting and touches upon how different formulations impact people’s opinions. </p>
  <p> The paper is divided into four main sections. The first section focuses on understanding the notion of bias. The second, on definitions of fairness and methods to mitigate bias. Here, we will focus on three mathematical fairness criteria. The third section discusses the perception of fairness with a starting point in the three criteria and highlights results from the survey. The last section sums up the remaining challenges and concludes the main points of this paper.   </p>
   
  <h1>The notion of bias</h1>
  <h3>It is the suit</h3>
  <p>
  … which gives identity. Kate Crawford presented in <d-cite key=" crawford2017keynote "></d-cite> a division of the notion of bias into the harm of allocation and harm of representation. Examining why representation bias can be harmful, invites us to think of the ‘CEO’ image search example – and the important role the suit for a super figure has. The point is that how we as groups or individuals are presented has an impact on how we are perceived by ourselves and others, which links to our identity <d-cite key=" crawford2017keynote "></d-cite>. Therefore, it does matter when there are no images of female CEOs. Other examples on representational bias are 1) facial recognition systems working poorer on faces with darker skin <d-cite key=" raji2019actionable"></d-cite>, 2) gender stereotypes in occupations in word embeddings <d-cite key=" bolukbasi2016man"></d-cite>, 3) the <a href="https://twitter.com/jackyalcine/status/615329515909156865">example</a>  of an Afro-American woman labelled as ‘gorilla’ by Google Photo. Potentially harm of allocation could also occur in recommender systems not showing adds about the new superhero film to certain profiles, e.g. females, even though gender is not a factor for whether you would like it or not. The issue is, therefore, not only that the performance level of systems differs between groups, but it is also how they fail that should raise concerns.
  </p>
  <figure  style="text-align: center;">
    <img src="./images/barbie.PNG" alt="barbies" style="width:400px;height:200px";>
    <figcaption>Barbies [TEKST] </figcaption>
  </figure>
   
  <h3> The division of powers </h3>
  <p>
  … can be denoted as an allocation problem. How resources and opportunities are allocated could potential be skewed and cause harm to a group, e.g. in loan applications <d-cite key="mukerjee2002multi "></d-cite> or parole decisions <d-cite key="dressel2018accuracy "></d-cite>. In an allocation problem, there is often one beneficial outcome, which we will denote as <i>positive</i>, versus a disadvantaged outcome denoted as the<i>negative</i> outcome. The focus is on different groups, which traditionally have been gender or race and wiht the aim to protect the members of the (minority-)group from discrimination or unfair treatment – we denote such notion of belonging to a group as <i>protected attributes</i>. A group subject to disadvantaging treatment is denoted the <i>unprivileged</i> group. When using data and statistics for decision-making, it can be required by law to not use any protected attributes in a system [REF]. In our super figure example it relates to not fit the classifier on gender, i.e. excluding <i>gender</i> from the set of input features. This is described as <i>fairness through unawareness</i> in the literature <d-cite key=" gajane2017formalizing "></d-cite>. However, it often does not solve the bias problem, since the information of a protected attribute can be latently present in other variables [REF]. In our example, imagine that gender is correlated with other variables such as  <i>weight</i> and <i>height</i>, i.e. given the weight and height it is possible to infer the gender.
  </p>
   
  <h3> Another dimension </h3>
  <p>
  …of the notion bias, would be not to look at the potential harm but at the source of its occurrence. The paper <d-cite key="mitchell2018prediction "></d-cite> defines the terms <i>statistical bias</i> and <i>societal bias</i>. The first term refers to errors in the collection of data, analysis, and model development in a way where reality is not represented properly. Societal bias refers to social structures that are, in fact, represented by the data but are perceived as unfair. This is an important distinction because treating one group disriminating might not be an issue in the model but caused by an underlying structure in society. However, it could also be the case that societal bias is manifesting itself into the model in the form of stereotyping <d-cite key=" carey2020blog "></d-cite> . The Issue is well explained in a blogpost by Valerie Carey <d-cite key=" carey2020blog "></d-cite> through a “money lending” scenario where the system is accused of discriminating against females. Take it as a fact for the sake of the example, that women on average have lower income than men which could be due to societal bias caused by discrimination in education or workplace. However, the question at hand is whether the model itself is discriminating when it gives fewer or smaller loans to the group of females. The important distinction is whether the model is fitting on income – which would be fair since it is reasonable to assume that income holds information about the ability to pay back a loan – or if the model instead is fitting on some proxy for gender, and thereby learning that females, in general, have lower income. The latter will result in an unfair treatment (stereotyping), since the fact of being female affects the chance of getting a loan independent of the income.   </p>
    <p>
  Both dimensions of understanding types of biases through either the impact or the occurrence find coherences in <d-cite key=" olteanu2019social "></d-cite> examination of social data where the authors present a diagram both representing the sources of occurrences of bias and its manifestations, as well as issues. Here, however, bias is divided into more subcategories, as also seen in <d-cite key=" mehrabi2019survey "></d-cite> who list 23 types. In the next paragraph, we will address how bias can occur in more detail without providing an extensive overview.
    </p>
  <h3> The villains are</h3>
  <p>
  … the data, the algorithm, and the user interactions <d-cite key=" mehrabi2019survey "></d-cite>. Data is usually the first suspect of bias introduced into a model, but all three inflict each other. To list some of the “offences” by data: 1) a skew in the data collections, 2) missingness in attributes, 3) a skew in human annotations or 4) an imbalance in examples. To stay in the super figure setting, one could imagine the case where the police historically have examined more male figures than females and therefore have found more villains among men, which then will be reinforced over time. Another case could be picturing missingness in self-reported attributes, where they could be missing for a reason related to a protected attribute like gender. Discrimination can also be induced into the data by human annotations. Unbalanced training data can also introduce a skew into a model. In our super figure toy data from Kaggle, we have an unbalanced dataset with fewer females than males, making it potentially harder for a model to generalise well on the classifications of female villains:
  </p>
  <div id="hero-villain-bar-figure" ></div>
  <p>
  An undesired skew can also be introduced into the model by the choice of the algorithm: imagine that one type of decision boundary is more suited for one group than the other. Last but not least, how the model is set into production and how users interact with it can in itself be a source of bias, e.g. following the models' suggestions in a skewed way <d-cite key="selbst2019fairness, green2019disparate "></d-cite>. Now, knowing that both humans, models, and the way models are being used can be biased, we invite you in <a href="#questionnaire-target-1">the question below</a> to think about how you prefer to be assessed if you were a super figure wanting to go to the party. We will return to this question in the section Perceived Algorithmic Fairness. But first, with our knowledge of how biases can occur, we need more formal definitions of what it means for a model to be fair. The next section, therefore, presents different ways of defining fairness and methods to achieve it.  
  </p>
  <div id="questionnaire-target-1" ></div>
  
  <h1>Defining fairness</h1>
  <h3> Joining the Justice League</h3>
  <p>
  … by stating what we mean with a just and fair model. Fairness is defined in the <a href="https://dictionary.cambridge.org/dictionary/english/fairness">Cambridge Dictionary </a> as <i>the quality of treating people equally or in a way that is right or reasonable</i>. Exactly, what it means to treat people right and how it can be operationalised is what the fairness literature is trying to determine. But it is no easy question: looking at the "CEO" image search example, where the first female revealed is a Barbie doll several rows down, it is not clear what a righteous result should yield. Kate Crawford raised the question if the proportion of male and female (and different ethnicities) images should resemble the current statistic of people contesting the job "CEO", or whether it should be what people <i>think </i> is the right proportion. After all, image search is contributing to shaping reality <d-cite key=" crawford2017keynote "></d-cite>.
  </p>
  <p>
   The other case from the introduction about criminal risk assessment is heavily used as an example in the literature, e.g. <d-cite key="berk2018fairness, green2019disparate, dressel2018accuracy"></d-cite>. It is an example of outcome fairness, where the goal is to determine how a desired outcome should be righteously distributed among groups or individuals. For example, <d-cite key= "angwin2016machine"> </d-cite> found the system to be biased against Afro-Americans, since the rate of <i>false negatives</i>, i.e. people who would not re-offend receiving wrongly a high risk score, was higher among black then white. However, in the response from the developers of COMPAS it was concluded that the system is not biased against black, since the <i>predictive parity</i> is the same between the two groups <d-cite key=" dieterich2016compas"></d-cite>, i.e. the fraction of subjects that correctly are given a positive outcome among all who were predicted a positive outcome, is the same between groups. The problem here is that these two measures are not necessarily compatible <d-cite key=" berk2018fairness"></d-cite> and it, therefore, opens a discussion of which of the two is most fair. 
  </p>
  <p>
  These two examples illustrate a challenge in defining and deciding what a fair model is before work can be done on mitigating undesired model behaviour. Clear definitions of biases are also essential to be able to spot and identify biases in the first place - without it is not clear what to look for when examining systems. Especially in the cases of representational harm, it can be hard to discover undesired model behaviour, as well as defining more righteous behaviour. However, an example of an attempt regarding stereotypes in word embeddings is made by <d-cite key="bolukbasi2016man"></d-cite>. Here the authors first compare occupational stereotype in word embeddings with human perception of occupational stereotype through an Amazon Turk survey and then mitigate the model bias according to the human perception. But it is not a method that scales in practice or remains robust through time and cultural change. On the other hand, regarding outcome fairness, a lot of work has been conducted on formalising fairness mathematically <d-cite key=" gajane2017formalizing"></d-cite>. In the literature over 70 <d-footnote id="d-footnote 1"> https://aif360.mybluemix.net/</d-footnote> different metrics have been defined, see e.g. <a href="https://aif360.mybluemix.net/">IBM’s AI Fairness 360</a>. In the rest of this section, we will look into mathematical fairness criteria for allocation problems and their trade-offs.  
  </p>
  <h3> Listing superpowers</h3>
  <p>
  … aka defining mathematical fairness criteria. Mathematical fairness criteria with respect to classification where a certain treatment or outcomes are the desired output can be grouped together in different ways. In the following <a href="#criteria-box">box</a>  we will highlight five ideas on formalizing fairness partly based on <d-cite key=" gajane2017formalizing, verma2018fairness, mitchell2018prediction, mehrabi2019survey"></d-cite> :
  </p>
  <div id="criteria-box" class="subgrid"></div>
  <h3 id="fairness-criteria"> Batman begins</h3>
  <p>… is by the way a great movie, and like Batman every super figure needs to learn to master their powers in the beginning. We will therefore start by understanding the first, and most known statistical measure of fairness, before we discuss the advantages and disadvantages of the statistical approach.  In the following we will explain the fairness metrics Demograhic Parity, Equalized Opportunity and Predictive Parity.</p>
  <p> As described in <a href="#criteria-box">the box above</a>, statistical measures are derived from the so called confusion matrix. The confusion matrix is a way to sort all examples in a dataset according to their true class and predicted class in a binary classification setting. The following table shows a confusion matrix and its components and describes the terminology needed for the definition of the specific statistical fairness criteria: </p>
  <figure>
    <figcaption align="justify"> 
  <b>Table 1:</b> Confusion matrix for a binary classification problem with a positive (typically defined as 1) and a negative (typically defined as 0) outcome. Hover or click on a particular cell to get a more detailed explanation of the corresponding term.
   </figcaption>
   <div id="confusion-matrix"></div>
  </figure>
  
  <h4><b>Demographic Parity </b></h4>
  <p>Demographic parity also called group fairness or statistical parity (<d-cite key="dwork2012fairness, zliobaite2015, verma2018fairness"></d-cite>) incorporates the idea, that non-discrimination between groups is achieved if the chance of getting a positive outcome is equalized acrossed groups. Given a binary classification task and two groups this can mathematically be formalized as: </p>
  <d-math block="">
  P(\hat{Y} |G=0)=P(\hat{Y} |G=1),
  </d-math block="">
    <p>where <d-math> \hat{Y} \in \{0,1 \}  </d-math> is the predictor’s outcome  <d-math> G \in \{0,1 \}  </d-math> is the group membership (the protected group variable)  and <d-math> P </d-math> is the conditional probability. It can also be thought of as achieving the same <i> Positive Rate </i> (PR) for each group which is per group calculated as the number of positive predictions divided by the total number of members in each group:</p>
  <d-math block="">
  PR = \frac{TP+FP}{TP+FP+FN+TN} .
  </d-math block="">
  
  <p>In our super figure example this means that the fraction of females and the fraction of males getting accepted to the party should be equalised, or in practice, be similar. In the survey, we investigated people’s comprehension of this fairness criterion. You can check your understanding as well: </p>
  
  <div id="tester-target-10" ></div>
   <div id="tester-target-11" ></div>
    <div id="tester-target-12" ></div>
  
  <p> The last question points to the critique to Demographic Parity by <d-cite key=" hardt2016equality"></d-cite>, where it is pointed out that the metric does not account for a case where the true outcome is correlated with the protected group. Such a case could force us to accept not-qualified figures in one group or dismiss qualified figures in the other group to achieve Demographic Parity. The criterion can, however, be justified, if we are looking at a <i> societal bias</i> we actively want to change. Imagine a case of young super figures getting admitted into an academy. Furthermore, assume that young male figures are less qualified than females. However, we want to enforce a policy of even acceptance rate, i.e. adjust for this skewness, since we believe it is a structural bias in society. (However, if the males are less qualified due to societal bias, it is not a good solution only to adjust the acceptance rate. There is a need to improve the males lesser qualification on acceptance time to match what is truly needed of skills. This could be done with, for example, extra classes in the beginning.)  </p>
   
  <p> By playing with the superhero data from Kaggle and the classifier train to distinguish “hero” or “villain”, we can on the evaluation set, look at the difference in the positive rate for males and females for different threshold values for the classification. By choosing different threshold values, you can achieve parity for the positive rate:
  </p>
   
   <figure>
    <div id="interactive-dp" ></div>
    <figcaption>In this illustration, different thresholds can be set for the female and male group by sliding each bar. If the two super figures raise their arms in cheers, a parity is reached for Demographic Parity. Here we have defined a parity to be sufficient if the positive rates are within 3 %-points of each other. This can e.g. be achieved by raising the threshold for the female group to  0.61. Another interactive illustration of achieving Demographic Parity in a loan application example can be examined on this <a href="http://research.google.com/bigpicture/attacking-discrimination-in-ml/">webpage </a> which is a companion to the paper by Hardt et al.  <d-cite key=" hardt2016equality"></d-cite>.  </figcaption>
  </figure>
   
  <h4> <b>Equalized Opportunity</b> </h4>
  <p>Equalized Opportunity and Equalized Odds is proposed by <d-cite key="hardt2016equality"></d-cite> to be an alternative criterion to Demographic Parity. The idea behind Equalized Opportunity is that people who in fact should receive a positive outcome have an equal chance of receiving it independent of their group membership. It is a relaxation of the Equalized Odds criteria, which can be formalized for a binary predictor and in the case of binary group membership as, </p>
  <d-math block="">
  P(\hat{Y} =1|Y=y,G=0)=P(\hat{Y}=1 |Y=y,G=1),
  </d-math block="">
    <p>where <d-math> y \in \{0,1\} </d-math> is the true class, <d-math> \hat{Y} \in \{0,1 \}  </d-math> is the the predictor’s outcome and <d-math> G \in \{0,1 \}  </d-math> is the group membership (the protected group variable). Relaxing the formulation to <d-math> y =1</d-math>, where a value of 1 represents the positive class, defines the criterion of Equalized Opportunity. This can also be formulated as requiring equal <i>True Positive Rate </i> (TPR) between the groups which is per group calculated as the number of correct positive predictions divided by the total number of actual positive members in the group:</p>
  <d-math block="">
  TPR = \frac{TP}{TP+FN} .
  </d-math block="">
  
  <p>  In our super figure example, this means that the chance of getting accepted to the party when you, in fact, are a hero should be the same for both males and females. You can verify your own understanding of Equalized Opportunity as well as see the response from the participants in the surveys with the following questions: </p>
    <div id="tester-target-13" ></div>
    <div id="tester-target-14" ></div>
    <div id="tester-target-15" ></div>
   <p>Looking at our superhero classifier´s performance on the evaluation set, we can experiment with satisfying the parity of the True Positive Rate by setting a different threshold value for the two groups. In <d-cite key=" hardt2016equality"></d-cite>  they formalized it as an optimization problem. This approach is an example of a post-processing algorithm to achieve fairness by adjusting the output labels after the model is trained.  As the question also asked, you can in this example see, that you can achieve Equalized Opportunity  without achieving Demographic parity:   </p>
  
  <figure>
    <div id="interactive-eq" ></div>
    <figcaption>Different thresholds can be set by sliding each bar for the male and female groups to try to achieve parity for Equalized Opportunity. If the large super figures raised the arms, the parity is fulfilled (within 3%-point), e.g. by raising the threshold for the female group to 0.56. However, below on the smaller figures, the impact on the Positive Rate for the two groups can be followed, and it can be examined if both measures can be achieved at the same time. Alternativ, see the illustrative example on this <a href="http://research.google.com/bigpicture/attacking-discrimination-in-ml/">webpage </a> <d-cite key=" hardt2016equality"></d-cite>.   </figcaption>
  </figure>
  
  <h4> <b>Predictive Parity</b> </h4>
  <p>
  Predictive parity, also described as outcome test, is a statistical measure which requires that the probability of a correct prediction - constrained to prediction as the positive class - is the same for all groups <d-cite key="chouldechova2017fair"></d-cite>. For a binary predictor it can be defined as </p>
  <d-math block="">
  P(Y =1|\hat{Y} =1,G=0)=P(Y=1 |\hat{Y}=1,G=1),
  </d-math block="">
    <p> <d-math> \hat{Y} \in \{0,1 \}  </d-math> is the predictor’s outcome ,<d-math> G \in \{0,1 \}  </d-math> is the group membership (the protected group variable) and <d-math> Y </d-math> is the true class, where a value of 1 indicates the positive class. It can also be thought of as achieving the same <i>Positive Predicted Value </i> (PPV) between the groups and calculated per group as the number of correct positive predictions divided by the total number of predicted positive members in a group:</p>
  <d-math block="">
  PPV =  \frac{TP}{TP+FP}  .
  </d-math block="">
  <p>For our super figure examples, this means that the chance of a correct prediction for figures allowed into the party should be the same for both males and females. Once again, you can try verifying your understanding and see people’s opinion with the following questions:</p>
  
    <div id="tester-target-16" ></div>
     <div id="tester-target-17" ></div>
    <div id="questionnaire-target-4" ></div>
    <div id="questionnaire-target-4.1" ></div>
  <p> Looking at our superhero classifier's performance on the evaluation set, we can experiment with satisfying the parity of the Positive Predictive Value by setting a different threshold value for the two groups and see how this affects both the parity for Demographic Parity and Equalized Opportunity.  The Precision-Recall curve shows the trade-offs between precision (PPV) and recall (TPR) for different thresholds values. </p> 
  <figure> 
  <div id="interactive-pp" ></div>
  <figcaption>This illustration shows that in this toy example, it is difficult to achieve a parity for the Positive Prective Value  by adjusting the threshold values (here by sliding the bars) for the classifier. It can be done with, e.g. male-threshold: 0.8  and female-threshold: 0.26, however, notice in this case how far the two other measures are from each other. A raise of the super figure's arms indicated a parity is reached within 3%-point. The Precision-Recall curve shows the trade-offs between precision (PPV) and recall (TPR) for different thresholds values for both the male and female group.</figcaption>
  </figure>
  
   
  <h3> The hero’s path</h3>
  <p>
  … is not going to be easy. In the previous section, we saw that it is not possible to simultaneously achieve Demographic Parity, Equalized Opportunity and Predictive Parity in the super figure example, unless we almost let no one into the club. This is not a special case. It is proven that, except in trivial cases, many common known notions of fairness are incompatible with each other and furthermore conflict with optimizing accuracy <d-cite key=" kleinberg2016inherent, chouldechova2017fair "></d-cite>. Others criticize that the statistical approach only gives guarantees for the average of a group and not for individuals or even <i>subgroups</i> <d-cite key=" kearns2018preventing "></d-cite>. Imagine a case where we only accept female figures from the DC Comics and males from the Marvel universe. Here, we could set up the scenario such that we comply with a fairness criterion for females versus males or DC figures versus Marvel figures, but without having a fair treatment for males from DC Comics or females from the Marvel universe. This problem is showcases by <d-cite key=" kearns2018preventing "></d-cite> both in form of a toy example (like here) and in the form of a real case. The authors suggest a framework to learn a fair classifier for a rich set of subgroups to alleviate the problem. However, it does not overcome the fact that a division of people into groups is required. Others have raised criticism of these statistical for being too static <d-cite key=" liu2018delayed,d2020fairness "></d-cite>. Liu et al. <d-cite key=" liu2018delayed "></d-cite> made one step simulations for a lending scenario and revealed that complying with Demographic Parity or Equalized Opportunity could lead the protected group to be worse off with respect to their underlying <i> credit scores </i> one step into the future. At the same time, a policy of unconstrained maximisation of profit for the “leading company”  would never lead to a scenario where the protected group was worse off one step in the future. Yet another critic comes from <d-cite key=" carey2020blog "></d-cite> which demonstrate that fairness metrics cannot be used to distinguish if a model is stereotyping members of a group. Since, the metrics do not reveal if the model fits on reasonable attributes, like income, or if the bias is not apparent in the model but introduced elsewhere in society, e.g. that women receive lower income than men. (See perhaps the previous subsection Another Dimension.). </p>
  <p>
    Despite its caveats, the strength of statistical measures of fairness is the easy way to check for a disparity and if applicable, they are also easy to achieve, e.g. by adjusting the classifier threshold. Both the verification and adjustment do not require assumptions of the data, in contrast to individual fairness and counterfactual fairness. However, choosing and interpreting any disparity metric requires understanding the bias occurrence and a discussion of whether an adjustment of the model to meet the parity is the desired outcome.     
   </p>
   <p>  We will return to the three statistical metrics worked with through this section in the section Perceived Algorithmic Fairness to examine the perception of them and investigate the comprehension based on the survey results. But first, the next paragraphs will show a process for mitigating biases.   </p>
  <h3>Circularize an orbit</h3>
  <p>… to defend it with all our superhero weapons. Until now this paper has listed different ways of defining fairness (see  <a href="#criteria-box">this box</a>)  and especially, there has been a focus on statistical measures in the form of three parity metrics. But now it is time to show the weapons for mitigating and monitoring biases. In the <a href="#ml-cycle">circle below</a>, we illustrate a process of developing a machine learning system with phases of where there can be intervened to mitigate biases. By clicking on the different phases, references are shown for dealing with biases at the current stages. Notice that the process is circular. Even though a system is in production and hence in the Model Monitoring phase, it might be beneficial to revisit the Data Gathering's phase to better deal with unwanted behaviour. 
  </p>
  <div id="ml-cycle" class="subgrid"></div>
  <p>
  </p>
  
   
  <h1>Perceived algorithmic fairness</h1>
  <h3>What is you favourite super power</h3>
  <p> … or in other words, which is your favourite fairness criterion?   
  </p>
  <div id="questionnaire-target-5" ></div>
  <p>
  The question of which statistical fairness measure people perceive as more fair has previously been addressed in the literature. The large bulk of studies in this regard have been motivated by the COMPAS debate between Equalized Opportunity and accuracy, as mentioned in the Introduction. Srivastava et al. <d-cite key="srivastava2019mathematical "></d-cite> showed through an Amazon Turk survey that Demographic Parity was perceived as most fair compared to metrics like Equalized Opportunity and Predictive Parity (in the paper presented as False Negative Rate and Discovery Rate). However, the survey was conducted by showing the participants ten figures together with the true label and the predicted label of two different algorithms, asking them to choose the most discriminating one. The question can be raised whether the participants fully understood the implications of the different “algorithms” and whether ten example figures are enough to generalise to a systematic bias. In Harrison et al. <d-cite key=" harrison2020empirical"></d-cite>, an Amazon Turk survey comparing the perception of different fairness measures was evaluated through a different design. Here, they showed participants histograms of two measures for two different models where one measure was equalised in one model and the other measure in the other model. Then participants were asked to choose between the models. However,  the same question can be raised again, whether the participants fully understood the implication of the parity in the models. The question about people’s comprehension of fairness metrics is addressed by Saha et al. <d-cite key="saha2019measuring "></d-cite>, which showed that comprehension can be measured through a multiple-choice survey. They found that in a hiring scenario, Equalized Opportunity was harder to understand than Demographic Parity, and that in general, comprehension was correlated with the participants’ education level. An interesting finding was a tendency of people with low comprehension score to express less negative sentiment towards a criterion. This survey’s design, where criteria are expressed as rules and questions are asked through multiple-choice, has inspired the survey conducted in this paper.Therefore, this survey’s objectives was likewise to measure the comprehension and link it to the perception of fairness, but also to examine the consistency in the answers and people´s self-reported understanding. The following subsections present the results of the survey. </p>
  <h4> A measure of comprehension</h4>
    <p> To measure the participants’ comprehension of each criterion, a score is computed as the percentage of correct answers to the comprehension questions. This yields an indication of the participants' understanding of the fairness criterion and it allows for a comparison of the comprehension between the three criteria. Figure X shows the distributions of the participants’ comprehension score on each fairness criterion. </p>
  <figure>
    <div class="row">
      <div class="column">
        <img src="./images/boxplot_comprehention_score.svg" alt="boxplot_comprehension_score" style="width:100%">
     
  </div>
      <div class="column">
        <img src="./images/boxplot_raported_score.svg" alt="boxplot_raported_score"style="width:100%">
     
  </div>
   
  </div>
   
  <figcaption style="text-align: left;"> <b> Figure x.</b> Left: Boxplot showing the distribution over participants’ computed comprehension score for each criterion. Using a Mann-Whitney U test to test the null hypothesis that the pairwise distribution of comprehension scores are equal yield the following p-values: DP-EO: <d-math>0.020<0.05 </d-math>, DP-PP: <d-math>0.007<0.05 </d-math> EO-PP: <d-math> 0.305 \nless 0.05 </d-math>. Hence, with a significant level of 0.05, we can reject the null hypothesis and conclude that DP is easier to understand than both EO and PP. The boxplot to the left shows the distribution over  participants’ self-reported comprehension score. We use the same test statistic to test for differences in distributions. We get the following p-values: DP-EO: <d-math> 0.010<0.05 </d-math>, DP-PP: <d-math> 0.000<0.05 </d-math> EO-PP: <d-math> 0.008<0.05 </d-math> . Hence, the difference is significant for all pairs.   </figcaption>
  </figure>
  
   <p> The distributions indicate that participants found Demographic Parity easiest to understand. A pairwise performed Mann-Whitney U test <d-cite key="mann1947test"></d-cite> reveals that the difference is significant between Demographic Parity and the two other criteria (p=0.020 for Equalized Opportunity and p=0.007 for Predictive Parity), but not between Equalized Opportunity and Predictive Parity. Therefore, we can conclude that Demographic Parity is easier to understand than both Equalized Opportunity and Predictive Parity in this survey.
  
  This supports the finding in <d-cite key="saha2019measuring "></d-cite> that Equalized Opportunity is harder to understand than Demographic Parity, albeit the different setting. </p>
  <p> We also asked the participants to self-report their understanding of each criterion after answering the comprehension questions using a five-point Likert-scale (see example in this <a href="#criteriaquestionnaire-target-4.1">question</a> ). Each point is assigned a numeric value where a higher value indicates a higher self-reported understanding. The distribution over the self-reported understanding is presented as boxplots in [Fig nr].  The pairwise comparisons all show a significant difference between self-reported comprehension (DP-EQ: p=0.010, DP-PP: p=0.000, EQ-PP: p=0.008), meaning that people report different levels of understanding for the three measures. Similar to the computed comprehension scores, the self-reported comprehension has the following order: 1) Demographic Parity, 2) Equalized Opportunity, 3) Predictive Parity.  
  </p>
  <p> It is examined if there is an association between the average self-reported and the average computed comprehension score. A calculation of the Spearman correlation <d-cite key="spearman1987proof"></d-cite> (<d-math> \rho=0.198</d-math>) and the associated p-value (<d-math>0.058</d-math>) do not show a strong correlation between the self-reported and computed comprehension score (see <a href="# Appendix-A"> Appendix A</a> ). Hence, we can not conclude a significant association between how people perceive their understanding and how well they actually understand the criteria. We did expect a strong correlation. Hence, the result indicates that it is difficult to discuss fairness if people are not aware of their own understanding. No clear trend towards over- or underrating oneself was found in the data. </p>
  
   
    <h4> The opinion towards the criteria </h4>
  <p>After the comprehension questions of each criterion, the participants were asked to state how fair they perceive the criterion on a five-point Likert-scale (see an example of the formulation in this <a href="#criteriaquestionnaire-target-4">question</a>), i.e. they were asked to evaluate the fairness of each criterion independent of the other criteria. The Likert-scale is transformed to numeric values, and the boxplots in [fig x] shows the different distributions. The fairness assessment resulted significantly in the following ranking: 1.) Equalized Opportunity, 2.) Predictive Parity, 3.) Demographic Parity.  </p>
  <figure 
  style="text-align: left;">
      <img src="./images/boxplot_fair_score.svg" alt=" boxplot_fair_score" style="width:75%";>
  <figcaption style="text-align: left;">Figure nr. Boxplots showing distributions over participants' reported opinion on each criterion when asked on a 5-point Likert-scale whether they think the criterion was fair to use. Using a Mann-Whitney U test to test the null hypothesis that the pairwise distribution of perception scores are equal yield for DP-EO a p-value of <d-math>0.000 < 0.05</d-math>, DP-PP a p-value of <d-math>0.001 < 0.05</d-math> and for EO-PP a p-value of <d-math>0.022 < 0.05</d-math>. With a significant level of 0.05, it can be concluded for all pairs that the distribution of perception scores are different.    </figcaption>
    </figure>
  
  
  <p>In <d-cite key="saha2019measuring"></d-cite>, Saha et al. found in their survey that people with low comprehension tend to have less negative opinion towards a fairness criterion. To investigate if the same trend is visible in the survey conducted here, we calculate the Spearman's correlation between perception and the computed comprehension score per criteria: </p>
  
  <figure>
    <figcaption>
      <b>Table 2</b>: Spearman correlation between comprehension and perception scores for three different statistical fairness criteria.
    </figcaption>
    <div id="spearman-correlation"></div>
    <figcaption style="text-align: center;"> *Significant correlation with a <i>p</i>-value below 0.05. </figcaption>
  </figure>
  
  <p> For Demographic Parity we can see a significant, negative correlation between comprehension and perception, i.e. the better people understand the criterion the less they perceive it as fair (see Table [x]). This is in accordance with the results from Saha et al. <d-cite key="saha2019measuring "></d-cite>. A weaker negative correlation can also be also seen for Predictive Parity, but it is not significant (<d-math>p=0.177 \nless 0.05<\d-math>). In contrast, for Equalized Opportunity, there is a small positive correlation between comprehension and perception  (not significant with <d-math>p=0.248 \nless 0.05<\d-math>), i.e. people who understand the criterion seem to like it more. </p>
   
   
   
  <h3>Superheroes live in a multiverse</h3>
    <p>
    … and so it seems to be the case with algorithmic fairness. Instead of only looking at which statistical measure is perceived as most fair, we need to broaden our understanding of the fairness universe. As we discussed earlier, bias can be introduced in the step of interacting with the system, and hence the interaction with the system can also affect the perceived fairness of the system. In general, there seems to be more to fairness than mathematical formulations.
  For example, <d-cite key="grgic2018beyond, grgic2016case"></d-cite> suggests shifting the focus from looking at distributive fairness to <i> procedural fairness</i>, which instead of looking at the outcome focusses on the process that leads to an outcome. They operationalise a part of the idea by examining which input features people perceive as fair when used in different scenarios. In our super figure example, we could easily imagine that this has a great impact on what is perceived as fair. Here, it is probably perceived fair to look at superpowers, since we do not want a dangerous cocktail. However, <i>weight</i> and <i>height</i> would seem less relevant and therefore discriminating to use, despite the fact that it could increase the model's accuracy.  Along these lines of the decision-making process matters for people's perception of fairness, work has been conducted to examine how different explanations styles of machine learning models impact the perception <d-cite key="binns2018s, dodge2019explaining"></d-cite>. One thing is an automatically generated explanation for a model or a model's output, another how actual formulation may impact opinions about fairness.  In this section, we touch on some more high-level questions of perceived fairness. We show through survey results that people may be sensitive to formulations and in some questions to a degree which demonstrates inconsistency in answers. Above, the survey results did not prove a statistical significance in the association between peoples self-reported understanding and what could be measured through their answers to comprehension questions. Both points to the need of being careful in the debate about fairness since formulations and wrongly estimated self-understanding may skew it.  
   
    </p>
    <h4> The preference for human or algorithmic judgement is subject to change</h4>
    <p>
   Human or machine - which would people prefer to be assessed by in different settings. This question was also raised by Harrison et al. who found a slight preference towards human judgment over a machine learning model <d-cite key="harrison2020empirical"></d-cite>. In our survey we investigated people’s opinion about using algorithmic or human judgement in the superhero setting, and explored how additional information can influence people’s opinion using the following three questions (the first question is replicated in this <a href="#questionnaire-target-1">question</a> and the two others are below): </p>
   
    <div id="questionnaire-target-2" ></div>
  
    <div id="questionnaire-target-3" ></div>
  
  <p> The answers show that, initially, the majority (77%) of participants preferred the option of a ‘human judgment supported by an algorithm’ when it is assumed that both the algorithm and the human act reasonably. However, this changes with the extra information that occasionally the human is biased against one sex. Under this assumption, the majority (56%) of participants preferred the algorithm, even though it is still, in general, assumed that both are acting reasonably. The third question assumes that the algorithm is  much more accurate than the human but also biased. This shifts the preference of the majority (71%) back to preferring 'human judgment with algorithmic support'. Our survey consists  of a small sample size, and the representativeness is not accounted for. Nevertheless, it is  interesting how relatively easy people’s opinion can be changed by adding some vague information about the system. In fact, 54% of the participants changed their choice throughout the three questions. </p>
   
  <h4> Participants are not consistent throughout their answers </h4>
  <p> In addition to people’s preference for human or algorithmic judgement, the survey also asked a more high-level question (see this <a href="#questionnaire-target-0">question</a>) regarding whether it is fair to use “a system that uses data and statistics” in the superhero setting. To this question, 22 out of 92 participants answered a clear “no”, but when asked about if they preferred human or algorithmic judgement, only 6 out of the 22 choose to solely trust the human. This number further decreased to 3 out of 22 when asked the question in <a href="#questionnaire-target-2">this question</a>. Although it can be argued that this does not reflect inconsistency depending on how the formulations are understood, it might point towards that people are easily affected by the formulation of the questions. However, an actual logical inconsistency is found when people are asked to rank the three fairness criteria discussed previously. First, the participants are asked to rank the three criteria formulated as rules shown in the answer-options in this <a href="#questionnaire-target-5"> question</a> according to importance for achieving fairness. Immediately thereafter they are asked to rank what would be the worst case of unfairness that could occur (see the<a href="#questionnaire-target-6"> question</a>) below. Here, the options are formulated as cases that  do not comply  or contradict with one of the three criteria – all three disadvantaging the same sex.</p>
  
    <div id="questionnaire-target-6" ></div>
  
  <p>We expect that the ranking of the three criteria should be the same for each participant between the two questions. However, 57% of the participants do not keep the same ranking between the three criteria, when they are formulated in different ways. We observed no significant difference between the comprehension of participants who rank consistently versus the group of participants who rank inconsistently (<d-math>p=0.061 \nless 0.05</d-math>), despite a slightly higher average comprehension of the consistent group (0.75 vs. 0.68 ). </p>
  
  <h1 id="challenges-ahead">The challenges ahead </h1>
  <h3>Leaving the superhero universe</h3>
  <p>
  … and zooming out of the academic sphere of mathematical formulations to look at the fairness challenges in industry. The fairness literature is focussing static settings which often does not resemble the challenges faced by practitioners and industry <d-cite key="holstein2019improving,chouldechova2020snapshot "></d-cite>.  Summarizing, for example,  some points from Holstein et al. <d-cite key="holstein2019improving "></d-cite>: 1) It is often assumed that the dataset is static and fairness can only be achieved  by changing the model, but in practice data collection can be changed at many practitioners point towards the idea of investigating bias already at that step. However, as seen in the <a href"#ml-cycle">machine learning circle</a>, the current literature mainly focuses on mitigating bias during the pre-processing or training steps.  2) The debiasing methods and metrics often do not apply in an actual context, e.g. because sensitive attributes can not be accessed  on an individual level. </p>
  <p> In general, the problems in practice usually include dynamic scenarios instead of one-shot classification tasks, like web search, chatbots and systems that employ online learning, reinforcement learning or bandit problems <d-cite key="holstein2019improving,chouldechova2020snapshot  "></d-cite>. For example,  Dwork and Ilvento <d-cite key="dwork2018group"></d-cite> show that even though models independently satisfy a group fairness metric, it is not given that a system that combines the models will.  There is a need for more methods to audit and monitor complex dynamic systems. One idea, proposed for models in an NLP setting  is an analogy to “software testing”, or more precisely behaviour testing or black-box testing <d-cite key=" ribeiro2020beyond "></d-cite>. The idea is to examine the model’s behaviour by providing a set of input-output tests without having access to the model itself. This can be used to check for fairness in different scenarios without the need of metrics but instead focusing on concrete example cases. For example, one could imagine a scenario where a text classification model for sentiment unwantedly associates specific names with the sentiment [REF?].
  </p>
  <h3> To be continued </h3>
  <p>
  … as every movie must come to an end so does our article. We have provided an overview of different ways to evaluate and define algorithmic fairness and bias, as well as, different methods of mitigating bias throughout the ML life cycle. We then discussed challenges with the currently proposed approaches with respect to people’s comprehension and perception, and application in practice. As seen in the previous <a href="#challenges-ahead">section</a>), the work on algorithmic fairness is far from done. We expect more work on algorithmic fairness in the future and hope that this article will inspire new research in the field. Like it is with superhero movies, the next one is already waiting around the corner.
  </p>
  
    
  

</d-article>




<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>

  <h3  id="Appendix-A">Appendix A: The survey</h3>
  <p><b>The conduction of the survey: </b> The design of the questionnaire can be seen on GitHub repository that corresponds to this paper [ref]. The survey was conducted in December 2020 - January 2021, and participants were volunteers recruited through social media focusing on accounts/pages with a Computer Science background. Participants were motivated by the fact that they contribute to research on Fair AI and by the possibility of winning a symbolic prize. In total, 92 people participated. The participants were asked for a few demographic information which yielded 40% male, 60% female, of which  88% are Danish. They were asked to self-report their level of experience in "statistics and/or machine learning" and in "ethics and/or legal philosophy" on a five-point Likert-scale. The participants were more experienced in statistics/machine learning with a median of 3 ("Moderately experienced") than in ethics and legal philosophy with a median of 2 ("Slightly experienced"). 
  </p>
  
  <p><b>The statistical test: </b> Different statistical tests are reported in the paper. This part of the appendix elaborates on the choice of tests and the assumptions required.
  A Mann-Whitney U test <d-cite key="mann1947test"></d-cite> is applied in several cases: 1) to test for differences in the computed and self-reported comprehension score between pairs of fairness criteria and 2) to test for differences in the opinion towards different criteria. This test static is chosen since each variable's observation does not seem to be drawn from a normal distribution. Normality is examined visually through boxplots, QQ-plots and by performing a Shapiro-Wilk test <d-cite key="shapiro1965analysis"></d-cite>. As an example, the computed comprehension score for Demographic Parity does not seem to come from a normal distribution when examining the two plots in Figure A1. Further, with a Shapiro-Wilk test statistics=0.837 and associated p-value=0.000, the null-hypothesis about normality can be rejected, and hence there is evidence that the data is not normally distributed. 
   </p>
  
  <figure>
    <div class="row">
      <div class="column">
        <img src="./images/DP_normal1.svg" alt="boxplot_comprehension_score" style="width:100%"> 
  </div>
      <div class="column">
        <img src="./images/DP_normal2.svg" alt="boxplot_raported_score"style="width:100%">
  </div>
  </div>
  <figcaption style="text-align: left;"> <b> Figure A1.</b> Boxplot and QQ-plot for visual inspecting if the data (the observation for the computed comprehension score for Demographic Parity) comes from a normal distribution. The plot suggests that the data is indeed not  normally distributed. </figcaption>
  </figure>
  
   <p> However, the variables fulfil the Mann-Whitney U test assumptions since they are ordinary and independent. We test the null hypotheses H0: The distribution of comprehension scores between two criteria is equal. We choose a significant level of <d-math> \alpha=0.05</d-math>. Obtaining a <i>p</i>-value under this level rejects the null hypothesis and concludes there is a significant difference in the distribution between the two tested variables. </p>
  <p> The paper also reports the Spearman rank-order correlation coefficient <d-cite key="spearman1987proof"></d-cite> between 1) computed and self-reported comprehension scores, and between 2) computed comprehension score and perception scores. This measure is chosen because the observations do not follow a normal distribution but they are ordinal. The associated test statistic is performed with the null hypothesis stating no correlation, and again with a significance level of <d-math> \alpha=0.05</d-math>.
  
</p>
  <h3 id="Appendix-B">Appendix B: The superhero classifier</h3>
   <p><b>Details of data preparation and training</b>

A simple classifier is trained to distinguish between hero and villain. The data is from the <a href="https://www.kaggle.com/claudiodavi/superhero-set"> Kaggle superhero dataset</a> which is a collection of information about super figures extracted from the <a href="https://www.superherodb.com/"> Superhero Database</a> . The data contains information about the super figures names, characteristics and possession of superpowers. Only super figures being either villain or hero and either male or female are considered in this setup. This yields 613 super figures which are randomly split into a train and a evaluation set (30%). The following input features are used in training: height, weight, one-hot encoding of 167 different powers and the names of the super figures transformed into count vectors of n-grams 3 and 4. In the variables <i> height </i> and <i> weight </i> negative values are replaced with mean values conditioned on the gender. Note the gender is not used as an explicit featuring during training, ensuring fairness through unawereness. In the example, gender is considered a protected attribute. The classifier is trained using the scikit-learns <d-cite key="scikit-learn"></d-cite> implementation of logistic regression with the default values, except the regularization strength parameter is set to C=0.4 and ‘balanced’ is used for ‘class_weights’ . The classifier obtained an accuracy on 0.72 and a macro-F1 on 0.66 on the evaluation set. 
</p>






  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>



<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>


</body>